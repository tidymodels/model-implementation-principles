[
["index.html", "Conventions for R Modeling Packages Chapter 1 Introduction", " Conventions for R Modeling Packages draft version compiled on 2018-09-15 Chapter 1 Introduction The S language has had unofficial conventions for modeling function, such as: using formulas to specify model terms, variable roles, and some statistical calculations basic infrastructure for creating design matrices (model.matrix), computing statistical probabilities (logLik), and commom analyses (anova, p.adjust, etc.) existing OOP classes like predict, update, etc. Despite a note written by an R core member in 2003, these conventions have never been formally required for packages and this has led to a a significant amount of between- and within-package heterogeneity. For example, the table below shows myriad methods for obtaining class probability estimates for a set of modeling functions: Function Package Code lda MASS predict(obj) glm stats predict(obj, type = &quot;response&quot;) gbm gbm predict(obj, type = &quot;response&quot;, n.trees) mda mda predict(obj, type = &quot;posterior&quot;) rpart rpart predict(obj, type = &quot;prob&quot;) Weka RWeka predict(obj, type = &quot;probability&quot;) logitboost LogitBoost predict(obj, type = &quot;raw&quot;, nIter) pamr.train pamr pamr.predict(obj, type = &quot;posterior&quot;) Note that many use different values of type to obtain the same output and one does not use the standard S3 predict method for the function. Also notice that two models require an extra parameter to make predictions and despite these parameters signifying the same idea (the ensemble size), different parameter names are used. There are myriad examples of inconsistencies in function design, return values, and other aspects of modeling functions. The goal of this document is to define a specification for creating functions and packages for new modeling packages. These are opinionated specifications but are meant to reflect reasonable positions for standards based on prior experience. A number of these guidelines are specific to the tidyverse (e.g. “Function names should use snake_case instead of camelCase.”). However, the majority are driven by common sense and good design principles (e.g. “All functions must be reproducible from run-to-run.”). The chapters in this document contain recommendations for different aspects of modeling functions. The items within are meant to be succinct and concise. In many cases, further details can be found in links to the Notes chapter. Examples and implementation details can be found there. Once these guidelines have been finalized, a set of GitHub repositories will be created to serve as templates that new package builders can use. "],
["general-conventions.html", "Chapter 2 General Conventions", " Chapter 2 General Conventions Code should follow the tidyverse style conventions All results must be reproducible from run-to-run. If there is a strong need for producing output during execution, there should be a verbose option that defaults to no output. {notes} "],
["function-interfaces.html", "Chapter 3 Function Interfaces 3.1 User-Facing Functions 3.2 Computational Functions", " Chapter 3 Function Interfaces We distinguish between “top-level”/“user-facing” api’s and “low-level”/“computational” api’s. The former being the interface between the users of the function (with their needs) and the code that does the estimation/training activities. When creating model objects, the computational code that fits the model should be decoupled from the interface code to specify the model. This allows for different interfaces to be used to specify the model that then pass common data structures on to the computational code. {note} 3.1 User-Facing Functions At a minimum, the top-level model function should be a generic with methods for data frames, formulas, and possibly recipes. These methods error trap bad arguments, format/encode the data, them pass it along to the lower-level computational code. Do not require users to create dummy variables from their categorical predictors. Provide a formula and/or recipe interface to your model to do this (see the next item) and other methods should error trap if qualitative data should not be subsequently used. These top-level functions should use proper S3 dispatch (e.g. appropriately invoke to foo.data.frame, foo.formula and so on). Only user-appropriate data structures should be accommodated for the user-facing function. The underlying computational code should make appropriate transformations to computationally appropriate formats/encodings. {note} Design the top-level code for humans. This includes using sensible defaults and protecting against common errors. Design your top-level interface code so that people will not hate you. {note} Parameters that users will commonly modify should be main arguments to the top-level function. Others, especially those that control computational aspects of the fit, should be contained in a control object. If your model passes ... to another modeling function, consider the names of your top-level function arguments to avoid conflicts with the argument names of the underlying function. {note} If possible, check to see if the arguments passed to ... are real arguments to the function(s) receiving the .... {note} Common arguments should use standardized names. {note} 3.2 Computational Functions A test set should never be required when fitting a model. If internal resampling is involved in the fitting process, there is a strong preference for using tidymodels infrastructure so that a common interface (and set of choices) can be used. If this cannot be done (e.g. the resampling occurs in C code), there should be some ability to pass in integer values that define the resamples. In this way, the internal sampling is reproducible. The same is true for other infrastructure (e.g. yardstick for performance measures, etc.) When possible, do not reimplement computations that have been done well elsewhere (tidy or not). For example, kernel methods should use the infrastructure in kernlab, exponential family distribution computations should use those in ?family etc. For modeling packages that use random numbers, setting the seed in R should control how random numbers are generated internally. At worst, a random number seed for non-R code (e.g. C, Java) should be an argument to the main modeling function. Computational code should (almost) always use X[ , ,drop = FALSE] when subsetting matrices (and non-tibble data frames) to make sure that the 2D structure is maintained. Use try or similar methods to avoid an uncontrolled errors in order to return intelligible errors. Using the call object for post-estimation activies is discouraged. {note} "],
["the-model-object.html", "Chapter 4 The Model Object", " Chapter 4 The Model Object Saving the call is discouraged. If it must be saved, it should be checked for size. In cases where the function is invoked by do.call(&quot;foo&quot;, bar), the data set may be embedded in the argument list bar. Unless explicitly required by the model, the training set should not be embedded in the model object (exceptions being models such as k-nearest neighbors). Retain the minimally sufficient objects in the model object. {note} "],
["print-and-summary-methods.html", "Chapter 5 Print and Summary Methods", " Chapter 5 Print and Summary Methods Every class should have a print method that gives a concise description of the object. The print method should invisibly return the original object. The number of significant digits should be an option and should use the global default. Printing the call is discouraged. summary methods are helpful but not required. These should create more verbose descriptions of the object. the print method for the summary object should follow the bullet-pointed remarks above for printing. "],
["model-predictions.html", "Chapter 6 Model Predictions 6.1 Input Data 6.2 Return Values", " Chapter 6 Model Predictions To be consistent with snake_case, new_data should be used instead of newdata. The function to produce predictions should be a class-specific predict method with arguments object, new_data, and possibly type. Other arguments, such as level, should be standardized. {note} The main predict method can internally defer to separate, unexported functions (predict_class, etc). type should also come from a set of pre-defined values such as type application response numeric predictions class hard class predictions prob class probabilities, survivor probabilities link glm linear predictor conf_int confidence intervals pred_int prediction intervals raw direct access to prediction function param_pred predictions across tuning parameters and should be validated using match.arg(). To determine whether or not to return standard errors for predictions, use a std_error argument that takes on TRUE/FALSE value. By default, do not report standard error or other measures of uncertainty, as these can be expensive to compute. Clearly document whether any standard errors are for confidence or prediction intervals. Other values should be assigned with consensus. 6.1 Input Data If new_data is not supplied, an error should be thrown. It should not default to an archived version of the training set contained in the model object. The data requirements for new_data should be the same as those for the orginal model fit function. The model outcome should never be required to be in new_data. new_data should be tolerant of extra columns. For example, if all variables are in some data frame dataset, predict(object, dataset) should immediately know which variables are required for prediction, check for their presence, and select only those from dataset before proceeding. The prediction code should work whether new_data has multiple rows or a single row. Predictions should not depend on which observations are present in new_data. {note}. When novel factor levels appear in the test set for factor predictors, the default behavior should be to throw an informative error. For models where this is a reasonable way to make predictions on novel factor levels, users need to explicitly specify that they want this behavior, and it’s good practice to message() for these prediction cases. 6.2 Return Values By default, new_data should not be returned by the prediction function. The return value is a tibble with the same number of rows as the data being predicted and in the same order. This implies that na.action should not affect the dimensions of the outcome object (i.e., it should be ignored). {note} The class of the tibble can be overloaded to accommodate specialized methods as long as basic data frame functionality is maintained. {note}. For observations with missing data such that a prediction cannot be generated, we recommend returning NA. The return tibble can contain extra attributes for values relevant to the prediction (e.g. level for intervals) but care should be taken to make sure that these attributes are not destroyed when standard operations are applied to the tibble (e.g. arrange, filter, etc.). Columns of constant values (e.g. adding level as a column) should be avoided. Specific cases: For univariate, numeric point estimates, the column should be named .pred. For multivariate numeric predictions (excluding probabilities), the columns should be named .pred_{outcome name}. Class predictions should be factors with the same levels as the original outcome and named .pred_class. For class probability predictions, the columns should be named the same as the factor levels, e.g., .pred_{level}, and there should be as many columns as factor levels. If interval estimates are produced (e.g. prediction/confidence/credible), the column names should be .pred_lower and .pred_upper. If a standard error is produced, the column should be named .std_error. For predictions that are not simple scalars, such as distributions or non-rectangular structures, the .pred column should be a list-column {note} In cases where the outcome is being directly predicted, the predictions should be on the same scale as the outcome. The same would apply to associated interval estimates. This is equivalent to type = &quot;response&quot; for generalized linear models and the like. Reasonable exceptions include estimation of the standard error of prediction (perhaps occurring on the link-level/scale of the linear predictors). "],
["standardized-argument-names.html", "Chapter 7 Standardized Argument Names 7.1 Data Arguments 7.2 Numerical Arguments 7.3 Statistical Quantities 7.4 Tuning Parameters", " Chapter 7 Standardized Argument Names Dot usage: If there is a possibility of argument name conflicts between the function and any arguments passed down through ..., it is strongly suggested that the argument names be prefixed with a dot (e.g. .data, .x, etc.) 7.1 Data Arguments na_rm: missing data handling. new_data: data to be predicted. weights: case weights. For .data.frame methods: x: predictors or generic data objects. y: outcome data. For .formula methods: formula: a y ~ x formula specifying the outcome and predictors. data: the data.frame to pull formula variables from. 7.2 Numerical Arguments times: the number of bootstraps, simulations, or other replications. 7.3 Statistical Quantities direction: the type of hypothesis test alternative. level: interval levels (e.g., confidence, credible, prediction, and so on). link: link functions for generalized linear models. 7.4 Tuning Parameters activation: the type of activation function between network layers. cost: a cost value for SVM models. Cp: The cost-complexity parameter in classical CART models. deg_free: a parameter for the degrees of freedom. degree: the polynomial degree. dropout: the parameter dropout rate. epochs: the number of iterations of training. hidden_units: the number of hidden units in a network layer. Laplace: the Laplace correction used to smooth low-frequency counts. learn_rate: the rate at which the boosting algorithm adapts from iteration-to-iteration. loss_reduction: The reduction in the loss function required to split further. min_n: The minimum number of data points in a node that are required for the node to be split further. mixture: the proportion of L1 regularization in the model. mtry: The number of predictors that will be randomly sampled at each split when creating the tree models. neighbors: a parameter for the number of neighbors used in a prototype model. num_comp: the number of components in a model (e.g. PCA or PLS components). num_terms: a nonspecific parameter for the number of terms in a model. This can be used with models that include feature selection, such as MARS. prod_degree: the number of terms to combine into interactions. A value of 1 implies an additive model. Useful for MARS models and some linear models. prune: a logical for whether a tree or set of rules should be pruned. rbf_sigma: the sigma parameters of a radial basis function. penalty: The amount of regularization used. In cases where different penalty types require to be differentiated, the names L1 and L2 are recommended. sample_size: the size of the data set used for modeling within an iteration of the modeling algorithm, such as stochastic gradient boosting. surv_dist: the statistical distribution of the data in a survival analysis model. tree_depth: The maximum depth of the tree (i.e. number of splits). trees: The number of trees contained in a random forest or boosted ensemble. In the latter case, this is equal to the number of boosting iterations. weight_func: The type of kernel function that weights the distances between samples (e.g. in a K-nearest neighbors model). "],
["parallel-processing.html", "Chapter 8 Parallel Processing", " Chapter 8 Parallel Processing If a model function is not thread-safe, the documentation should clearly state that it cannot be run in parallel. Parallel processing should always be implemented on the longest running operation. {note} Parallel processing must be explictly requested by the user. Provide an argument to specify the amount (e.g. number of cores if appropriate) and default the function to run sequentially. Computations should be easily reproducible, even when run in parallel. Parallelism should not be an excuse for irreproducibility. {note} Computational code in other languages (e.g. Cpp, etc.) should pull from R’s random number streams so that settin the seed prior to invoking these routines ensures reproducibility. "],
["notes.html", "Chapter 9 Notes", " Chapter 9 Notes writing output To write messages, cat() or message() can be used. Their differences: cat() goes to standard out and message() goes to standard error. When used inside of some parallel processing backends, cat() output is obscured inside of the RStudio IDE. Unless coded otherwise, cat() uses the usual formatting whereas the default formatting for message() is different. For this code: library(emojifont) message(&quot;Help! I&#39;m trapped in the well!!&quot;) cat(&quot;No, you&#39;re not&quot;, emoji(&#39;roll_eyes&#39;), &quot;\\n&quot;) the Rstudio IDE output is: and basic terminal output is: This post may also be helpful in deciding. ↩️ decoupling functions For example, for some modeling method, the function foo would be the top-level api that users experience and some other function (say compute_foo_fit) is used to do the computations. This allows for different interfaces to be used to specify the model that pass common data structures to compute_foo_fit. ↩️ appropriate data structures For example: Categorical data should be in factor variables (as opposed to binary or integer representations). Rectangular data structures that allows for mixed data types should always be used even when the envisioned application would only make sense when the data are single type. For strictly univariate response models, vector input is acceptable for the outcome argument. Censored data should follow the survival::Surv convention. ↩️ top-level design examples For example: Suppose a model can only fit numeric or two-class outcomes and uses maximum likelihood. Instead of providing the user with a distribution option that is either “Gaussian” or “Binomial”, determine this from the type of the data object (numeric or factor) and set internally. This prevents the user from making a mistake that could haven been avoided. If a model parameter is bounded by some aspect of the data, such as the number of rows or columns, coerce bad values to this range (e.g. mtry = min(mtry, ncol(x))) with an accompanying warning when this is critical information. ↩️ avoid common parameter names For example, control is the type of argument used in many functions so have a function specific argument (e.g. foo_control) is advisable in these situations. ↩️ Checking of ... The conflicted package can be used to solve this issue. ↩️ standardize names For example, many functions use some variation of level for confidence levels (as opposed to alpha). The names in Chapter 7 are preferable for the specific context. ↩️ discussing new names A good venue for this discussion is RStudio Community ↩️ avoid computing on call objects Historically, the call object that results from a model fit was considered a good method to capture details of the existing model fit. This object could be parsed and manipulated for the purpose of continuing or revising the model between function calls. This can be problematic because the call object is relative to the environment and the call itself is agnostic to its original environment. If the call is changed and re-evaluated, the current environment may be inappropriate. This could result in errors due to the required objects not being exposed in the current environment. Note that the internals of common modeling functions, such as lm, do exactly this. However, these manipulations occur within the function call to that the environment is the same. ↩️ minimal object retention For example, if summary statistics are computed on the model object, derive them on-the-fly during the execution of the summary method. Avoid saving these statistics to the model object unless they are related to ephemeral values generated during the model fit but do not need to be retained afterwards. ↩️ consistent number of rows This should enable the ability to bind the results with the original data: cbind(data, predict(model, data)) # or predict(model, data) %&gt;% bind_cols(data) ↩️ extra tibble classes Adding a new class to a tibble object might cause subsequent errors to dplyr operations. This code is a good example for how to maintain dplyr compatibility. ↩️ list column output Some examples: posterior distirbutions If a posterior distribution is returned for each sample, each element of the list column can be a tibble with as many rows as samples in the distribution. multiple hyperparameters When a predict method produces values over multiple tuning parameter values (e.g. glmnet), the list column elements have rows for every tuning parameter value combination that is being changed (e.g. lambda in glmnet). survivor probability predictions In time to event models, where survivor probabilities are produced for values of time, the return tibble has a column for .time. Other column names should conform to the standards here (e.g. .pred_lower if intervals are also returned). As an example from ?flexsurv::flexsurvreg: library(flexsurv) data(ovarian) fitg &lt;- flexsurvreg(formula = Surv(futime, fustat) ~ age, data = ovarian, dist = &quot;gengamma&quot;) For each new sample, this model can make probabilistic predictions at a number of user-specified time points. preds &lt;- summary(fitg, newdata = ovarian[1:2, &quot;age&quot;, drop = FALSE], t = c(100, 200, 300)) preds ## age=72.3315 ## time est lcl ucl ## 1 100 0.7988680 0.49687521 0.9599880 ## 2 200 0.4849180 0.13567502 0.7952071 ## 3 300 0.2784226 0.02610516 0.6402210 ## ## age=74.4932 ## time est lcl ucl ## 1 100 0.7278338 0.370927844 0.9351591 ## 2 200 0.3856737 0.050992138 0.7276700 ## 3 300 0.1964611 0.002097565 0.5869867 A list of data frames is not very helpful. Better would be a data frame with a list column, where every element is a data frame with the predictions in the tidy format: tidy_preds ## # A tibble: 2 x 1 ## .pred ## &lt;list&gt; ## 1 &lt;tibble [3 × 4]&gt; ## 2 &lt;tibble [3 × 4]&gt; # predictions on the first subject at 3 time points tidy_preds$.pred[[1]] ## # A tibble: 3 x 4 ## .time .pred .pred_lower .pred_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.799 0.497 0.960 ## 2 200 0.485 0.136 0.795 ## 3 300 0.278 0.0261 0.640 If a single data frame is needed, it is easy to make the conversion: tidyr::unnest(tidy_preds) ## # A tibble: 6 x 4 ## .time .pred .pred_lower .pred_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.799 0.497 0.960 ## 2 200 0.485 0.136 0.795 ## 3 300 0.278 0.0261 0.640 ## 4 100 0.728 0.371 0.935 ## 5 200 0.386 0.0510 0.728 ## 6 300 0.196 0.00210 0.587 percentile predictions When using a quantile regression, one might make the median the default that is predicted. If multiple percentiles are requested, then .pred would be a tibble with a column for the predictions and another for the percentile. ↩️ where to parallelize? For example, if a random forest model is being created the parallelism could simultaneously fit the trees in the ensemble or parallelize the search for optimal splits within each tree. All other things being equal, the former is preferred. ↩️ controlling random numbers in parallel workers One simple approach when parallelism is conducted using R subprocesses (e.g. via foreach or futures) is to pre-compute seeds and have a seed argument to the function being invoked. The function can immediately set the seed from the argument when it begins execution. ↩️ predictions shouldn’t depend on new_data Sometimes spline expansions or similar terms can behave oddly when packages haven’t been fully loaded. For example: fit &lt;- lm(mpg ~ disp + hp + splines::ns(drat, 2), mtcars) hp &lt;- head(predict(fit, mtcars)) ph &lt;- predict(fit, head(mtcars)) all.equal(hp, ph) ## [1] &quot;Mean relative difference: 0.1161239&quot; Loading the splines package fixes this: library(splines) fit &lt;- lm(mpg ~ disp + hp + ns(drat, 2), mtcars) hp &lt;- head(predict(fit, mtcars)) ph &lt;- predict(fit, head(mtcars)) all.equal(hp, ph) ## [1] TRUE A general way to check for errors like this is to check that predict() and head() commute. ↩️ "]
]
